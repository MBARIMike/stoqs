{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Data in Columns for Data Science\n",
    "*Pivot the row-based data in a STOQS database to fit into a column-based dataframe*\n",
    "\n",
    "This Notebook explores options raised by this [GitHub Issue](https://github.com/stoqs/stoqs/issues/837#issuecomment-763176111). We want to be able to efficiently extract mass quantities of data from a STOQS database and have it organized for efficient data analysis and visualization using modern data frame orientied tools.\n",
    "\n",
    "Executing this Notebook requires a personal STOQS server.  It can be run from either a Docker installation or from a development Vagrant Virtual Machine. \n",
    "\n",
    "### Docker Instructions\n",
    "Install and start the software as \n",
    "[detailed in the README](https://github.com/stoqs/stoqs#production-deployment-with-docker). (Note that on MacOS you will need to modify settings in your `docker-compose.yml` and `.env` files &mdash; look for comments referencing 'HOST_UID'.)\n",
    "\n",
    "Then, from your `$STOQS_HOME/docker` directory start the Jupyter Notebook server - you can query from the remote database or from a copy that you've made to your local system: \n",
    "\n",
    "#### Option A: Query from MBARI's master database\n",
    "Start the Jupyter Notebook server pointing to MBARI's master STOQS database server. (Note: firewall rules limit unprivileged access to such resources):\n",
    "\n",
    "    docker-compose exec \\\n",
    "        -e DATABASE_URL=postgis://everyone:guest@kraken.shore.mbari.org:5432/stoqs \\\n",
    "        stoqs stoqs/manage.py shell_plus --notebook\n",
    "\n",
    "#### Option B: Query from your local Docker Desktop\n",
    "Restore the `stoqs_canon_october2020` database from MBARI's server onto your local database and start the Jupyter Notebook server using the default DATABASE_URL, which should be your local system, also make sure that your Docker Desktop has at least 16 GB or RAM allocated to it:\n",
    "\n",
    "    cd $STOQS_HOME/docker\n",
    "    docker-compose exec stoqs createdb -U postgres stoqs_canon_october2020\n",
    "    curl -k https://stoqs.shore.mbari.org/media/pg_dumps/stoqs_canon_october2020.pg_dump | \\\n",
    "        docker exec -i stoqs pg_restore -Fc -U postgres -d stoqs_canon_october2020\n",
    "    docker-compose exec stoqs stoqs/manage.py shell_plus --notebook\n",
    "\n",
    "###  Opening this Notebook\n",
    "Following execution of the `stoqs/manage.py shell_plus --notebook` command a message is displayed giving a URL for you to use in a browser on your host, e.g.:\n",
    "\n",
    "    http://127.0.0.1:8888/?token=<a_token_generated_upon_server_start>\n",
    "\n",
    "In the browser window opened to this URL navigate to this file (`select_data_in_columns_for_data_science.ipynb`) and open it. You will then be able to execute the cells and modify the code to suit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Prevent SynchronousOnlyOperation exceptions\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "\n",
    "# Use a recent database available at DATABASE_URL\n",
    "db = 'stoqs_canon_october2020'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. Perform a straight forward query using the STOQS data model, collecting all the sea_water_temperature and sea_water_salinity data into dictionaries keyed by platform name. This is to examine the landscape of data we are querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we collect temperatures and salinities that are properly associated\n",
    "# we will first find all the Platforms that have T & S and then from each Measurement\n",
    "# from the Platform collect the temperatures and salinities into lists for plotting.\n",
    "# Assume that Platforms that have sea_water_salinity also have sea_water_temperature.\n",
    "platforms = (ActivityParameter.objects.using(db)\n",
    "                              .filter(parameter__standard_name='sea_water_salinity')\n",
    "                              .values_list('activity__platform__name', flat=True)\n",
    "                              .distinct().order_by('activity__platform__name'))\n",
    "temps = {}\n",
    "salts = {}\n",
    "for platform in platforms:\n",
    "    print(f\"Collecting data for: {platform:23}\", end=' ')\n",
    "    mps = (MeasuredParameter.objects.using(db)\n",
    "           .filter(measurement__instantpoint__activity__platform__name=platform))\n",
    "    \n",
    "    temps[platform] = (mps.filter(parameter__standard_name='sea_water_temperature')\n",
    "                          .values_list('datavalue', flat=True))\n",
    "    salts[platform] = (mps.filter(parameter__standard_name='sea_water_salinity')\n",
    "                          .values_list('datavalue', flat=True))\n",
    "    print(f\"#temps: {len(temps[platform]):6}  #salts: {len(salts[platform]):6}\", end='')\n",
    "    if len(temps[platform]) != len(salts[platform]):\n",
    "        print(' - not equal')\n",
    "    else:\n",
    "        print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a T/S plots of data from all the platforms\n",
    "\n",
    "import pylab as plt\n",
    "for platform in temps.keys():\n",
    "    ##print(f\"Plotting data from {platform}\")\n",
    "    if len(temps[platform]) == len(salts[platform]):\n",
    "        plt.scatter(temps[platform], salts[platform])\n",
    "        plt.title(platform)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use the same kind of self-join query used for selecting data for Parameter-Parameter plots. A sample SQL statement was copied from the STOQS UI and then modified to select sea_water_temperature and sea_water_salinity from all platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_multp = '''SELECT DISTINCT stoqs_measuredparameter.id,\n",
    "                stoqs_platform.name,\n",
    "                stoqs_instantpoint.timevalue,\n",
    "                stoqs_measurement.depth,\n",
    "                mp_salt.datavalue AS salt,\n",
    "                mp_temp.datavalue AS temp\n",
    "FROM stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_activity ON (stoqs_instantpoint.activity_id = stoqs_activity.id)\n",
    "INNER JOIN stoqs_platform ON (stoqs_activity.platform_id = stoqs_platform.id)\n",
    "INNER JOIN stoqs_measurement m_salt ON m_salt.instantpoint_id = stoqs_instantpoint.id\n",
    "INNER JOIN stoqs_measuredparameter mp_salt ON mp_salt.measurement_id = m_salt.id\n",
    "INNER JOIN stoqs_parameter p_salt ON mp_salt.parameter_id = p_salt.id\n",
    "INNER JOIN stoqs_measurement m_temp ON m_temp.instantpoint_id = stoqs_instantpoint.id\n",
    "INNER JOIN stoqs_measuredparameter mp_temp ON mp_temp.measurement_id = m_temp.id\n",
    "INNER JOIN stoqs_parameter p_temp ON mp_temp.parameter_id = p_temp.id\n",
    "WHERE (p_salt.standard_name = 'sea_water_temperature')\n",
    "  AND (p_temp.standard_name = 'sea_water_salinity')\n",
    "  AND stoqs_platform.name IN ({})'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\"\n",
    "db = 'stoqs_canon_october2020'\n",
    "platforms = (ActivityParameter.objects.using(db)\n",
    "                              .filter(parameter__standard_name='sea_water_salinity')\n",
    "                              .values_list('activity__platform__name', flat=True)\n",
    "                              .distinct())\n",
    "plats = ''\n",
    "for platform in platforms:\n",
    "    ##if platform == 'makai' or platform == 'pontus':\n",
    "    ##    continue\n",
    "    plats += f\"'{platform}',\"\n",
    "plats = plats[:-2] + \"'\"\n",
    "sql = sql_multp.format(plats)\n",
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from django.db import connections\n",
    "\n",
    "# It takes about 2 minutes to read about 25 million rows from the STOQS database. \n",
    "%time df = pd.read_sql_query(sql, connections[db])\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the Parquet file takes about 4 seconds\n",
    "%time df1.to_parquet('all_plats.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Parquest file takes about 2 seconds\n",
    "%time df1b = pd.read_parquet('all_plats.parquet')\n",
    "df1b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach could be used in a general way to extract all Parameters for each Platform by dynamically generating the SQL and executing it. We do need more scalable methods than `.read_sql_query()` and `.to_parquet()` which need to read and write all the data in to and out of allocated random access memory. \n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Use Brent's trimSTOQS program to convert the MeasuredParameter Data Access output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It takes about 4 minutes to read in 0.17 million CSV rows and convert using trimSTOQS\n",
    "! time wget https://stoqs.mbari.org/stoqs_canon_october2020/api/measuredparameter.csv?measurement__instantpoint__activity__platform__name=dorado \\\n",
    "    -q -O - | /srv/stoqs/contrib/trimSTOQS trimSTOQS parameter__name --separator=, > october2020_dorado_parms.cvs\n",
    "\n",
    "%time df2 = pd.read_csv('/srv/stoqs/contrib/trimSTOQS/october2020_dorado_parms.cvs')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of this approach is that all parameters get transformed into the columns we want. The disadvantage is that it takes a long time to extract the data in CSV format.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Do a direct Postgresql query to transform the data, perhaps using the [crosstab() function](https://stackoverflow.com/questions/3002499/postgresql-crosstab-query/11751905#11751905). \n",
    "\n",
    "Need to do this on the database first:\n",
    "\n",
    "    % docker-compose exec postgis psql -U postgres  \n",
    "    postgres=# \\c stoqs_canon_october2020\n",
    "    stoqs_canon_october2020=# CREATE EXTENSION IF NOT EXISTS tablefunc;\n",
    "    CREATE EXTENSION\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''SELECT stoqs_instantpoint.timevalue, stoqs_measurement.id,\n",
    "\t   standard_name, datavalue as datavalue FROM public.stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_activity ON (stoqs_instantpoint.activity_id = stoqs_activity.id)\n",
    "INNER JOIN stoqs_platform ON (stoqs_activity.platform_id = stoqs_platform.id)\n",
    "INNER JOIN stoqs_parameter ON (stoqs_measuredparameter.parameter_id = stoqs_parameter.id)\n",
    "WHERE stoqs_platform.name LIKE 'dorado' \n",
    "ORDER BY stoqs_measurement.id'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from django.db import connections\n",
    "\n",
    "# The crosstab query takes about 40 seconds\n",
    "%time df3 = pd.read_sql_query(sql, connections['default'])\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df3.id, [df3.standard_name, df3.datavalue])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''SELECT\n",
    "\t* \n",
    "FROM\n",
    "\tcrosstab('SELECT to_char(stoqs_instantpoint.timevalue, ''YYYY-MM-DD\"T\"HH24:MI:SS\"Z\"'') as timevalue, \n",
    "\t                  name as name, datavalue as datavalue FROM public.stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_parameter ON (stoqs_measuredparameter.parameter_id = stoqs_parameter.id)\n",
    "ORDER BY stoqs_instantpoint.timevalue, name') \n",
    "AS final_result(\"timevalue\" TEXT, \"temperature (Celsius)\" FLOAT, \"salinity\" FLOAT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from django.db import connections\n",
    "\n",
    "# The crosstab query takes about 40 seconds\n",
    "%time df3 = pd.read_sql_query(sql, connections['default'])\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See: https://datashader.org/\n",
    "import datashader as ds, pandas as pd, colorcet\n",
    "\n",
    "cvs = ds.Canvas(plot_width=300, plot_height=300)\n",
    "agg = cvs.points(df.loc[df['name'].isin(('pontus','makai'))], 'temp', 'salt')\n",
    "img = ds.tf.shade(agg, cmap=colorcet.fire, how='eq_hist')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: https://datashader.org/getting_started/Pipeline.html\n",
    "import holoviews as hv\n",
    "from holoviews.operation.datashader import datashade\n",
    "hv.extension(\"bokeh\")\n",
    "datashade(hv.Points(df, kdims=['temp', 'salt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See: http://holoviews.org/user_guide/Large_Data.html\n",
    "from holoviews.operation.datashader import rasterize\n",
    "##ropts = dict(tools=[\"pan,wheel_zoom,box_zoom\"], height=380, width=330, colorbar=True, colorbar_position=\"bottom\")\n",
    "ropts = dict(height=380, width=330, colorbar=True, colorbar_position=\"bottom\")\n",
    "%time hv.Layout([rasterize(hv.Points(df.loc[df['name'] == p],kdims=['temp', 'salt'])).opts(**ropts).relabel(p)for p in platforms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIP below this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See: https://stackoverflow.com/questions/50755586/how-to-loop-large-parquet-file-with-generators-in-python\n",
    "from fastparquet import ParquetFile\n",
    "pf = ParquetFile('myfile.parq')\n",
    "for df in pf.iter_row_groups():\n",
    "    process sub-data-frame df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = df.hvplot.points('temp', 'salt', title=platform,\n",
    "                                datashade=True, dynspread=True, \n",
    "                                frame_height=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_sql = '''SELECT\n",
    "\t* \n",
    "FROM\n",
    "\tcrosstab( 'SELECT to_char(stoqs_instantpoint.timevalue, ''YYYY-MM-DD\"T\"HH24:MI:SS\"Z\"'') as timevalue, \n",
    "\t                  stoqs_parameter.name as name, datavalue as datavalue FROM public.stoqs_measuredparameter\n",
    "INNER JOIN stoqs_measurement ON (stoqs_measuredparameter.measurement_id = stoqs_measurement.id)\n",
    "INNER JOIN stoqs_instantpoint ON (stoqs_measurement.instantpoint_id = stoqs_instantpoint.id)\n",
    "INNER JOIN stoqs_activity ON (stoqs_instantpoint.activity_id = stoqs_activity.id)\n",
    "INNER JOIN stoqs_platform ON (stoqs_activity.platform_id = stoqs_platform.id)\n",
    "INNER JOIN stoqs_parameter ON (stoqs_measuredparameter.parameter_id = stoqs_parameter.id)\n",
    "WHERE stoqs_platform.name IN (''makai_ESPmv1_filtering'')\n",
    "ORDER BY stoqs_instantpoint.timevalue, stoqs_parameter.name') \n",
    "AS final_result(\"timevalue\" TEXT, \"temperature (Celsius)\" FLOAT, \"salinity\" FLOAT)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
